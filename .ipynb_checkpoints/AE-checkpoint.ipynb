{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 6,
   "id": "eb6e2a85",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.dataset_utils as dataset\n",
    "import utils.train_utils as train\n",
    "\n",
    "from os import path\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['figure.dpi'] = 500"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 7,
   "id": "6e6a6c58",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector_norm(X):\n",
    "    return (X.T / np.sqrt((X**2).sum(axis=1))).T"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 8,
   "id": "819362b7",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE = True\n",
    "file_location = \"../data/Raman_Mouse/approximated/\"\n",
    "file_location_org = \"../data/Raman_Mouse/corrected_4_wavenumbers/\"\n",
    "filenames = np.load(f\"{file_location}FileNames.npy\")\n",
    "with open(f'{file_location}Sample_labels.pickle', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "vector_norm = unit_vector_norm if NORMALIZE else lambda x: x\n",
    "    \n",
    "data = []\n",
    "for f in filenames:\n",
    "    raw = np.load(f\"{file_location_org}{f.split('.')[0]}.npy\")\n",
    "    raman = np.load(f\"{file_location}{f.split('.')[0]}_raman.npy\")\n",
    "    photo = np.load(f\"{file_location}{f.split('.')[0]}_photoluminescence.npy\")\n",
    "#     data.append((vector_norm(raw.reshape(-1,raw.shape[-1])).reshape(raw.shape),\n",
    "#                  vector_norm(raman.reshape(-1,raman.shape[-1])).reshape(raman.shape),\n",
    "#                  vector_norm(photo.reshape(-1,photo.shape[-1])).reshape(photo.shape), \n",
    "#                  labels[f]))\n",
    "    data.append((raw,\n",
    "                 raman,\n",
    "                 vector_norm(photo.reshape(-1,photo.shape[-1])).reshape(photo.shape), \n",
    "                 labels[f]))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 9,
   "id": "484b262d",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, num_input_channels=1, base_channel_size=3, latent_dim=130, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = Decoder(num_input_channels, base_channel_size, latent_dim)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, x.shape[-1])\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat.reshape(x.shape[0],-1)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2*c_hid, 4*c_hid, kernel_size=3, padding=1, stride=5), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(4*65*c_hid, 2*latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear_raman = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*65*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        \n",
    "        self.net_raman = nn.Sequential(\n",
    "            nn.ConvTranspose1d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=4, padding=1, stride=5), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose1d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose1d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.ReLU() \n",
    "        )\n",
    "        \n",
    "#         self.linear_photo = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, latent_dim),\n",
    "#             act_fn(),\n",
    "#             nn.Linear(latent_dim, 2*65*c_hid),\n",
    "#             act_fn(),\n",
    "#         )        \n",
    "        \n",
    "#         self.net_photo = nn.Sequential(\n",
    "#             nn.ConvTranspose1d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=4, padding=1, stride=5), # 4x4 => 8x8\n",
    "#             act_fn(),\n",
    "#             nn.Conv1d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose1d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "#             act_fn(),\n",
    "#             nn.Conv1d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose1d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "#             nn.ReLU() \n",
    "#         )\n",
    "        \n",
    "        self.linear_photo = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*latent_dim),\n",
    "            act_fn(),\n",
    "            nn.Linear(2*latent_dim, 10*latent_dim),\n",
    "            act_fn(),\n",
    "        )\n",
    "        \n",
    "#         self.net_photo == nn.Sequential(\n",
    "#             nn.ConvTranspose1d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=4, padding=1, stride=5), # 4x4 => 8x8\n",
    "#             nn.ReLU() \n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 2, -1)\n",
    "        raman = x[:,0,:]\n",
    "        photo = x[:,1,:]\n",
    "        raman = self.linear_raman(raman)\n",
    "        raman = raman.reshape(raman.shape[0], -1, 65)\n",
    "        raman, photo = self.net_raman(raman), self.linear_photo(photo)\n",
    "        photo = photo.reshape(photo.shape[0], 1, -1)\n",
    "        return torch.cat((raman, photo), -1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 2, -1)\n",
    "#         raman = x[:,0,:]\n",
    "#         photo = x[:,1,:]\n",
    "#         raman, photo = self.linear_raman(raman), self.linear_photo(photo)\n",
    "#         raman, photo = raman.reshape(raman.shape[0], -1, 65), photo.reshape(photo.shape[0], -1, 65)\n",
    "#         raman, photo = self.net_raman(raman), self.net_photo(photo)\n",
    "#         return torch.cat((raman, photo), -1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 10,
   "id": "01db3a1d",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedClassifier(BaseEstimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        _use_cuda = torch.cuda.is_available() and kwargs['cuda']\n",
    "        if _use_cuda:\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.device = torch.device('cuda' if _use_cuda else 'cpu')        \n",
    "        print(f\"device: {self.device}\")\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.model = AE(**self.kwargs).to(self.device)\n",
    "\n",
    "        parameters = filter(lambda x: x.requires_grad, self.model.parameters())\n",
    "        self.optimizer = optim.Adam(parameters, lr=0.001)                \n",
    "        train_loader, test_loader = dataset.load_splitdata(data, self.kwargs['batch_size'])\n",
    "        \n",
    "        for epoch in range(self.kwargs['epochs']):\n",
    "            if path.exists(f\"AE_model_epoch{epoch}.pt\"):\n",
    "                print(f\"epoch {epoch} is already trained\")\n",
    "                if not path.exists(f\"AE_model_epoch{epoch+1}.pt\"):\n",
    "                    self.model = torch.load(f\"AE_model_epoch{epoch}.pt\")\n",
    "                continue \n",
    "            if epoch == 1:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.0005  \n",
    "            elif epoch == 5:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.0001\n",
    "            elif epoch == 10:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.00005\n",
    "            print('-'*50)\n",
    "            print('Epoch {:3d}/{:3d}'.format(epoch+1, self.kwargs['epochs']))\n",
    "            start_time = datetime.now()\n",
    "            train.train(self.model, self.optimizer, train_loader, self.kwargs['loss_func'], self.kwargs['acc_func'], self.kwargs['log_step'], self.device)\n",
    "            end_time = datetime.now()\n",
    "            time_diff = relativedelta(end_time, start_time)\n",
    "            print('Elapsed time: {}h {}m {}s'.format(time_diff.hours, time_diff.minutes, time_diff.seconds))\n",
    "            train.test(self.model, test_loader, self.kwargs['loss_func'], self.kwargs['loss_func'], self.device)\n",
    "            torch.save(self.model, f\"AE_model_epoch{epoch}.pt\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict transforms the data into the reference space. Min weight should be 0 or higher then 'min_weight'\n",
    "        The error is the NMSE, where the MSE is normalised by the signal strength. \n",
    "        error.shape = X.shape[0], so for each data point the error is calculated.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "#         self.model(X)\n",
    "        \n",
    "#         return RCA_vector"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 11,
   "id": "2d6509fc",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "loss2 = nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "# loss3 = nn.KLDivLoss(size_average=None, reduce=None, reduction='batchmean', log_target=False)\n",
    "# both closely related\n",
    "loss4 = nn.HuberLoss(reduction='mean', delta=100.0)\n",
    "loss5 = nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n",
    "\n",
    "def loss_func(y, y_):\n",
    "    raman, photo, _ = y\n",
    "    raman = raman.to(y_.device)\n",
    "    photo = photo.to(y_.device)\n",
    "#     x = torch.cat((raman, photo), -1)\n",
    "    return loss4(y_[:,:1300], raman) + 10000 * loss1(y_[:,1300:], photo)\n",
    "\n",
    "def acc_func(y, y_):\n",
    "    y_clone = torch.clone(y_)\n",
    "    y_clone = y_clone.cpu().detach().numpy()\n",
    "    plt.plot(y_clone[0][:1300])\n",
    "    plt.plot(y[0][0])\n",
    "    plt.ylim(-1, 800)\n",
    "    plt.show()\n",
    "    plt.plot(y_clone[0][1300:])\n",
    "    plt.plot(y[1][0])\n",
    "    plt.ylim(-0.0001, 0.05)\n",
    "#     plt.ylim(-1, 5000)    \n",
    "    plt.show()\n",
    "    return loss_func(y, y_)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 12,
   "id": "8324d793",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "kwargs = {'batch_size': BATCH_SIZE,\n",
    "          'cuda': True,\n",
    "          'log_step': 500,\n",
    "          'epochs': EPOCHS,\n",
    "          'loss_func': loss_func,\n",
    "          'acc_func' : acc_func,\n",
    "          'bias': True,\n",
    "          'base_channel_size': 130, \n",
    "          'latent_dim': 130\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": null,
   "id": "c703feef",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "device: cuda\n",
      "epoch 0 is already trained\n",
      "epoch 1 is already trained\n",
      "epoch 2 is already trained\n",
      "epoch 3 is already trained\n",
      "epoch 4 is already trained\n",
      "epoch 5 is already trained\n",
      "epoch 6 is already trained\n",
      "epoch 7 is already trained\n",
      "epoch 8 is already trained\n",
      "epoch 9 is already trained\n",
      "epoch 10 is already trained\n",
      "epoch 11 is already trained\n",
      "epoch 12 is already trained\n",
      "epoch 13 is already trained\n",
      "epoch 14 is already trained\n",
      "epoch 15 is already trained\n",
      "epoch 16 is already trained\n",
      "epoch 17 is already trained\n",
      "epoch 18 is already trained\n",
      "epoch 19 is already trained\n",
      "epoch 20 is already trained\n",
      "epoch 21 is already trained\n",
      "epoch 22 is already trained\n",
      "epoch 23 is already trained\n",
      "epoch 24 is already trained\n",
      "epoch 25 is already trained\n",
      "epoch 26 is already trained\n",
      "epoch 27 is already trained\n",
      "epoch 28 is already trained\n",
      "epoch 29 is already trained\n",
      "epoch 30 is already trained\n",
      "epoch 31 is already trained\n",
      "epoch 32 is already trained\n",
      "epoch 33 is already trained\n",
      "epoch 34 is already trained\n",
      "epoch 35 is already trained\n",
      "epoch 36 is already trained\n",
      "epoch 37 is already trained\n",
      "epoch 38 is already trained\n",
      "epoch 39 is already trained\n",
      "epoch 40 is already trained\n",
      "epoch 41 is already trained\n",
      "epoch 42 is already trained\n",
      "epoch 43 is already trained\n",
      "epoch 44 is already trained\n",
      "epoch 45 is already trained\n",
      "epoch 46 is already trained\n",
      "epoch 47 is already trained\n",
      "epoch 48 is already trained\n",
      "epoch 49 is already trained\n",
      "epoch 50 is already trained\n",
      "epoch 51 is already trained\n",
      "epoch 52 is already trained\n",
      "epoch 53 is already trained\n",
      "epoch 54 is already trained\n",
      "epoch 55 is already trained\n",
      "epoch 56 is already trained\n",
      "epoch 57 is already trained\n",
      "epoch 58 is already trained\n",
      "epoch 59 is already trained\n",
      "epoch 60 is already trained\n",
      "epoch 61 is already trained\n",
      "epoch 62 is already trained\n",
      "epoch 63 is already trained\n",
      "epoch 64 is already trained\n",
      "epoch 65 is already trained\n",
      "epoch 66 is already trained\n",
      "epoch 67 is already trained\n",
      "epoch 68 is already trained\n",
      "epoch 69 is already trained\n",
      "epoch 70 is already trained\n",
      "epoch 71 is already trained\n",
      "epoch 72 is already trained\n",
      "epoch 73 is already trained\n",
      "epoch 74 is already trained\n",
      "epoch 75 is already trained\n",
      "epoch 76 is already trained\n",
      "epoch 77 is already trained\n",
      "epoch 78 is already trained\n",
      "epoch 79 is already trained\n",
      "epoch 80 is already trained\n",
      "epoch 81 is already trained\n",
      "epoch 82 is already trained\n",
      "epoch 83 is already trained\n",
      "epoch 84 is already trained\n",
      "epoch 85 is already trained\n",
      "epoch 86 is already trained\n",
      "epoch 87 is already trained\n",
      "epoch 88 is already trained\n",
      "epoch 89 is already trained\n",
      "epoch 90 is already trained\n",
      "epoch 91 is already trained\n",
      "epoch 92 is already trained\n",
      "epoch 93 is already trained\n",
      "epoch 94 is already trained\n",
      "epoch 95 is already trained\n",
      "epoch 96 is already trained\n",
      "epoch 97 is already trained\n",
      "epoch 98 is already trained\n",
      "epoch 99 is already trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SupervisedClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
=======
      "device: cpu\n"
     ]
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
    }
   ],
   "source": [
    "rvc = SupervisedClassifier(**kwargs)\n",
    "rvc.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "d51d9910",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
=======
   "id": "4183e89f",
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.6"
=======
   "version": "3.8.8"
>>>>>>> 3738e58672d48980c2fc4997768e987592328b12
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
