{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1340abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.dataset_utils as dataset\n",
    "import utils.train_utils as train\n",
    "import preprocessing.GSP as GSP\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26aadd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"../data/Raman_Mouse/\"\n",
    "filenames = np.load(f\"{file_location}FileNames.npy\")\n",
    "with open(f'{file_location}Sample_labels.pickle', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "data = []\n",
    "for f in filenames:\n",
    "    data.append((np.load(f\"{file_location}{f}\"), labels[f]))\n",
    "\n",
    "# assume linear increase in wavelength \n",
    "wavelength = list(range(data[0][0].shape[2]))\n",
    "shape = data[0][0].shape\n",
    "\n",
    "# raman_data = []\n",
    "# photo_data = []\n",
    "# for x in data:\n",
    "#     tmp = copy.copy(x.reshape(-1, x.shape[-1]))\n",
    "#     tmp1, tmp2 = GSP.split_Raman_af(GSP.smoothing(tmp), wavelength)\n",
    "#     raman_data.append(tmp1)\n",
    "#     photo_data.append(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88febd05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-04d233d1c8a3>:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  data = np.array(data)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-04d233d1c8a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mPATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_liver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Master_thesis/utils/dataset_utils.py\u001b[0m in \u001b[0;36mload_liver\u001b[0;34m(data, batch_size, sample_size)\u001b[0m\n\u001b[1;32m     69\u001b[0m     ])\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtensor_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "PATCH_SIZE = 5\n",
    "data = np.array(data)\n",
    "train_loader, test_loader = dataset.load_liver(data, BATCH_SIZE, PATCH_SIZE)\n",
    "\n",
    "for batch_idx, (x, *_) in enumerate(train_loader):\n",
    "    print(batch_idx, x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d77302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedClassifier(BaseEstimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = {}\n",
    "        self.ae_kwargs = {}\n",
    "        self.k_means_kwargs = {}                \n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "        _use_cuda = torch.cuda.is_available() and kwargs['cuda']\n",
    "        if _use_cuda:\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.device = torch.device('cuda' if _use_cuda else 'cpu')        \n",
    "        \n",
    "    def fit(self, x, **kwargs):\n",
    "        self.set_params(**kwargs)\n",
    "        X = unit_vector_norm(x)\n",
    "        \n",
    "        ###################### Autoencoder ################################\n",
    "        self.model = AutoEncoder(**self.ae_kwargs).to(self.device)\n",
    "        \n",
    "        parameters = filter(lambda x: x.requires_grad, self.model.parameters())\n",
    "        self.optimizer = optim.Adam(parameters)        \n",
    "        train_loader, test_loader = dataset.load_liver(X, self.kwargs['batch_size'])\n",
    "        \n",
    "        for epoch in range(self.kwargs['epochs']):\n",
    "            print('-'*50)\n",
    "            print('Epoch {:3d}/{:3d}'.format(epoch+1, self.kwargs['epochs']))\n",
    "            start_time = datetime.now()\n",
    "            train.train(self.model, self.optimizer, train_loader, self.kwargs['loss_func'], self.kwargs['log_step'], self.device)\n",
    "            end_time = datetime.now()\n",
    "            time_diff = relativedelta(end_time, start_time)\n",
    "            print('Elapsed time: {}h {}m {}s'.format(time_diff.hours, time_diff.minutes, time_diff.seconds))\n",
    "            loss = train.test(self.model, test_loader, self.kwargs['loss_func'], self.device)\n",
    "            print('Validation| bits: {:2.2f}'.format(loss), flush=True)    \n",
    "          \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            W = self.model.encode(dataset.load_liver_all(X).to(self.device))\n",
    "        self.z = W\n",
    "        W = W.cpu().detach().numpy()\n",
    "        \n",
    "        ###################### clustering ################################        \n",
    "        self.clusters = cluster.KMeans(**self.k_means_kwargs).fit(W)\n",
    "        self.clusters = self.clusters.labels_\n",
    "        \n",
    "        one_hot = np.zeros((X.shape[0], self.kwargs['n_clusters']), dtype=bool)\n",
    "        one_hot[range(X.shape[0]), self.clusters] = 1\n",
    "                          \n",
    "        ###################### reference spectra ################################\n",
    "        self.reference_spectra_ = unit_vector_norm(np.array([np.abs(x_) for i, x_ in enumerate(one_hot.T @ X)]))\n",
    "        self.ref_org = np.array([x[one_hot[:,i],:].mean(axis=0) for i in range(self.kwargs['n_clusters'])])\n",
    "               \n",
    "        # Return the classifier\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict transforms the data into the reference space. Min weight should be 0 or higher then 'min_weight'\n",
    "        The error is the NMSE, where the MSE is normalised by the signal strength. \n",
    "        error.shape = X.shape[0], so for each data point the error is calculated.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        X = unit_vector_norm(X)\n",
    "        \n",
    "        ###################### RCA ################################           \n",
    "        RCA_vector = np.array([optimize.nnls(self.reference_spectra_.T, X[i,:])[0] for i in range(X.shape[0])])\n",
    "\n",
    "        return RCA_vector\n",
    "    \n",
    "    def get_reference_vectors(self):\n",
    "        return self.reference_spectra_\n",
    "\n",
    "    def get_org_reference_vectors(self):\n",
    "        return self.ref_org    \n",
    "    \n",
    "    def get_params(self, deep=False):\n",
    "        return self.kwargs\n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        self.kwargs.update(kwargs)\n",
    "        self.k_means_kwargs.update({k:v  for k,v in kwargs.items() if k in list(inspect.signature(cluster.KMeans).parameters.keys())})     \n",
    "        self.ae_kwargs.update({k:v  for k,v in kwargs.items() if k in list(inspect.signature(AutoEncoder).parameters.keys())})     \n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
