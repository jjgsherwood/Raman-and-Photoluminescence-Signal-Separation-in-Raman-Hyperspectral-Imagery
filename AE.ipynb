{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.dataset_utils as dataset\n",
    "import utils.train_utils as train\n",
    "\n",
    "from os import path\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "plt.rcParams['figure.dpi'] = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_vector_norm(X):\n",
    "    return (X.T / np.sqrt((X**2).sum(axis=1))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE = True\n",
    "file_location = \"../data/Raman_Mouse/approximated/\"\n",
    "file_location_org = \"../data/Raman_Mouse/corrected_4_wavenumbers/\"\n",
    "filenames = np.load(f\"{file_location}FileNames.npy\")\n",
    "with open(f'{file_location}Sample_labels.pickle', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "vector_norm = unit_vector_norm if NORMALIZE else lambda x: x\n",
    "    \n",
    "data = []\n",
    "for f in filenames:\n",
    "    raw = np.load(f\"{file_location_org}{f.split('.')[0]}.npy\")\n",
    "    raman = np.load(f\"{file_location}{f.split('.')[0]}_raman.npy\")\n",
    "    photo = np.load(f\"{file_location}{f.split('.')[0]}_photoluminescence.npy\")\n",
    "#     data.append((vector_norm(raw.reshape(-1,raw.shape[-1])).reshape(raw.shape),\n",
    "#                  vector_norm(raman.reshape(-1,raman.shape[-1])).reshape(raman.shape),\n",
    "#                  vector_norm(photo.reshape(-1,photo.shape[-1])).reshape(photo.shape), \n",
    "#                  labels[f]))\n",
    "    data.append((raw,\n",
    "                 raman,\n",
    "                 vector_norm(photo.reshape(-1,photo.shape[-1])).reshape(photo.shape), \n",
    "                 labels[f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, num_input_channels=1, base_channel_size=3, latent_dim=130, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = Decoder(num_input_channels, base_channel_size, latent_dim)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 1, x.shape[-1])\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat.reshape(x.shape[0],-1)\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2*c_hid, 4*c_hid, kernel_size=3, padding=1, stride=5), # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), # Image grid to single feature vector\n",
    "            nn.Linear(4*65*c_hid, 2*latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            - num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "            - base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "            - latent_dim : Dimensionality of latent representation z\n",
    "            - act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear_raman = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*65*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        \n",
    "        self.net_raman = nn.Sequential(\n",
    "            nn.ConvTranspose1d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=4, padding=1, stride=5), # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv1d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose1d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv1d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose1d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "            nn.ReLU() \n",
    "        )\n",
    "        \n",
    "#         self.linear_photo = nn.Sequential(\n",
    "#             nn.Linear(latent_dim, latent_dim),\n",
    "#             act_fn(),\n",
    "#             nn.Linear(latent_dim, 2*65*c_hid),\n",
    "#             act_fn(),\n",
    "#         )        \n",
    "        \n",
    "#         self.net_photo = nn.Sequential(\n",
    "#             nn.ConvTranspose1d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=4, padding=1, stride=5), # 4x4 => 8x8\n",
    "#             act_fn(),\n",
    "#             nn.Conv1d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose1d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 => 16x16\n",
    "#             act_fn(),\n",
    "#             nn.Conv1d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "#             act_fn(),\n",
    "#             nn.ConvTranspose1d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 => 32x32\n",
    "#             nn.ReLU() \n",
    "#         )\n",
    "        \n",
    "        self.linear_photo = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*latent_dim),\n",
    "            act_fn(),\n",
    "            nn.Linear(2*latent_dim, 10*latent_dim),\n",
    "            act_fn(),\n",
    "        )\n",
    "        \n",
    "#         self.net_photo == nn.Sequential(\n",
    "#             nn.ConvTranspose1d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=4, padding=1, stride=5), # 4x4 => 8x8\n",
    "#             nn.ReLU() \n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 2, -1)\n",
    "        raman = x[:,0,:]\n",
    "        photo = x[:,1,:]\n",
    "        raman = self.linear_raman(raman)\n",
    "        raman = raman.reshape(raman.shape[0], -1, 65)\n",
    "        raman, photo = self.net_raman(raman), self.linear_photo(photo)\n",
    "        photo = photo.reshape(photo.shape[0], 1, -1)\n",
    "        return torch.cat((raman, photo), -1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 2, -1)\n",
    "#         raman = x[:,0,:]\n",
    "#         photo = x[:,1,:]\n",
    "#         raman, photo = self.linear_raman(raman), self.linear_photo(photo)\n",
    "#         raman, photo = raman.reshape(raman.shape[0], -1, 65), photo.reshape(photo.shape[0], -1, 65)\n",
    "#         raman, photo = self.net_raman(raman), self.net_photo(photo)\n",
    "#         return torch.cat((raman, photo), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedClassifier(BaseEstimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        _use_cuda = torch.cuda.is_available() and kwargs['cuda']\n",
    "        if _use_cuda:\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "        self.device = torch.device('cuda' if _use_cuda else 'cpu')        \n",
    "        print(f\"device: {self.device}\")\n",
    "        \n",
    "    def fit(self, data):\n",
    "        self.model = AE(**self.kwargs).to(self.device)\n",
    "\n",
    "        parameters = filter(lambda x: x.requires_grad, self.model.parameters())\n",
    "        self.optimizer = optim.Adam(parameters, lr=0.001)                \n",
    "        train_loader, test_loader = dataset.load_splitdata(data, self.kwargs['batch_size'])\n",
    "        \n",
    "        for epoch in range(self.kwargs['epochs']):\n",
    "            if path.exists(f\"AE_model_epoch{epoch}.pt\"):\n",
    "                print(f\"epoch {epoch} is already trained\")\n",
    "                if not path.exists(f\"AE_model_epoch{epoch+1}.pt\"):\n",
    "                    self.model = torch.load(f\"AE_model_epoch{epoch}.pt\")\n",
    "                continue \n",
    "            if epoch == 1:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.0005  \n",
    "            elif epoch == 5:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.0001\n",
    "            elif epoch == 10:\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = 0.00005\n",
    "            print('-'*50)\n",
    "            print('Epoch {:3d}/{:3d}'.format(epoch+1, self.kwargs['epochs']))\n",
    "            start_time = datetime.now()\n",
    "            train.train(self.model, self.optimizer, train_loader, self.kwargs['loss_func'], self.kwargs['acc_func'], self.kwargs['log_step'], self.device)\n",
    "            end_time = datetime.now()\n",
    "            time_diff = relativedelta(end_time, start_time)\n",
    "            print('Elapsed time: {}h {}m {}s'.format(time_diff.hours, time_diff.minutes, time_diff.seconds))\n",
    "            train.test(self.model, test_loader, self.kwargs['loss_func'], self.kwargs['loss_func'], self.device)\n",
    "            torch.save(self.model, f\"AE_model_epoch{epoch}.pt\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predict transforms the data into the reference space. Min weight should be 0 or higher then 'min_weight'\n",
    "        The error is the NMSE, where the MSE is normalised by the signal strength. \n",
    "        error.shape = X.shape[0], so for each data point the error is calculated.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "#         self.model(X)\n",
    "        \n",
    "#         return RCA_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = nn.MSELoss(size_average=None, reduce=None, reduction='mean')\n",
    "loss2 = nn.L1Loss(size_average=None, reduce=None, reduction='mean')\n",
    "# loss3 = nn.KLDivLoss(size_average=None, reduce=None, reduction='batchmean', log_target=False)\n",
    "# both closely related\n",
    "loss4 = nn.HuberLoss(reduction='mean', delta=100.0)\n",
    "loss5 = nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean', beta=1.0)\n",
    "\n",
    "def loss_func(y, y_):\n",
    "    raman, photo, _ = y\n",
    "    raman = raman.to(y_.device)\n",
    "    photo = photo.to(y_.device)\n",
    "#     x = torch.cat((raman, photo), -1)\n",
    "    return loss4(y_[:,:1300], raman) + 10000 * loss1(y_[:,1300:], photo)\n",
    "\n",
    "def acc_func(y, y_):\n",
    "    y_clone = torch.clone(y_)\n",
    "    y_clone = y_clone.cpu().detach().numpy()\n",
    "    plt.plot(y_clone[0][:1300])\n",
    "    plt.plot(y[0][0])\n",
    "    plt.ylim(-1, 800)\n",
    "    plt.show()\n",
    "    plt.plot(y_clone[0][1300:])\n",
    "    plt.plot(y[1][0])\n",
    "    plt.ylim(-0.0001, 0.05)\n",
    "#     plt.ylim(-1, 5000)    \n",
    "    plt.show()\n",
    "    return loss_func(y, y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "\n",
    "kwargs = {'batch_size': BATCH_SIZE,\n",
    "          'cuda': True,\n",
    "          'log_step': 500,\n",
    "          'epochs': EPOCHS,\n",
    "          'loss_func': loss_func,\n",
    "          'acc_func' : acc_func,\n",
    "          'bias': True,\n",
    "          'base_channel_size': 130, \n",
    "          'latent_dim': 130\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvc = SupervisedClassifier(**kwargs)\n",
    "rvc.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
